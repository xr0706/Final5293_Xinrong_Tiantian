[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "5293 Final Porject",
    "section": "",
    "text": "1 Introduction\nIn this project, we investigate the robustness of model explanations across three dimensions: randomness in model training, size of training data, and consistency between model-agnostic explanations and inherently interpretable models. We use Random Forest and Logistic Regression to test these three aspects, and the process is structured in a 12 steps:\n[Step1 Chepter]\nRepeatability - Random Forest\nRepeatability Analysis - Random Forest\nRepeatability - Logistic Regression\nRepeatability Analysis - Logistic Regression\n[Step2 Chepter]\nChange Training Dataset Size - Random Forest\nChange Training Dataset Size Analysis - Random Forest\nChange Training Dataset Size - Logistic Regression\nChange Training Dataset Size Analysis - Logistic Regression\n[Step3 Chepter]\nConsistency - Random Forest\nConsistency Analysis - Random Forest\nConsistency - Logistic Regression\nConsistency Analysis - Logistic Regression",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "step1.html",
    "href": "step1.html",
    "title": "2  Step 1",
    "section": "",
    "text": "2.1 Repeatability - Random Forest\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(DALEX)\n\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\n\nCode\nlibrary(iml)\nlibrary(mlbench)\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nCode\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\nlime_results &lt;- list()\nshap_results &lt;- list()\n\nfor (i in 1:10) {\n  set.seed(i)\n  rf_model &lt;- randomForest(diabetes ~ ., data = train_data, ntree = 100)\n\n  # LIME\n  explainer_lime &lt;- DALEX::explain(\n    rf_model,\n    data = train_data[, -ncol(train_data)],\n    y = train_data$diabetes\n  )\n  lime_expl &lt;- predict_parts(\n    explainer_lime,\n    new_observation = test_data[1, -ncol(test_data)],\n    type = \"break_down\"\n  )\n  lime_results[[i]] &lt;- lime_expl\n\n  # SHAP\n  X &lt;- train_data[, -ncol(train_data)]\n  predictor &lt;- iml::Predictor$new(\n    rf_model,\n    data = X,\n    y = train_data$diabetes,\n    type = \"prob\"\n  )\n  shap &lt;- iml::Shapley$new(predictor, x.interest = test_data[1, -ncol(test_data)])\n  shap_results[[i]] &lt;- shap$results\n}\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3471545 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3489431 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3501789 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3503252 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3474309 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3496748 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3470244 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3438699 , max =  0.98  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3456748 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  randomForest  (  default  )\n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.randomForest  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package randomForest , ver. 4.7.1.2 , task classification (  default  ) \n  -&gt; model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -&gt; model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -&gt; model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -&gt; model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -&gt; predicted values  :  numerical, min =  0 , mean =  0.3498374 , max =  0.99  \n  -&gt; residual function :  difference between y and yhat (  default  )\n\n\nWarning in Ops.factor(y, predict_function(model, data)): '-' not meaningful for\nfactors\n\n\n  -&gt; residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!\nCode\n# LIME\n\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:DALEX':\n\n    explain\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\nlime_df &lt;- lapply(1:10, function(i) {\n  df_i &lt;- lime_results[[i]]\n  data.frame(\n    seed = i,\n    variable = df_i$variable_name,\n    contribution = df_i$contribution\n  )\n}) |&gt; bind_rows()\n\n\nlime_df_clean &lt;- lime_df |&gt;\n  filter(variable != \"intercept\" & variable != \"\") |&gt;\n  mutate(variable = factor(variable))  \n\nlime_summary_clean &lt;- lime_df_clean |&gt;\n  group_by(variable) |&gt;\n  summarise(\n    mean_contribution = mean(contribution),\n    sd_contribution = sd(contribution),\n    .groups = \"drop\"\n  )\n\nprint(lime_summary_clean)\n\n\n# A tibble: 8 × 3\n  variable mean_contribution sd_contribution\n  &lt;fct&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n1 age                0.0457          0.0117 \n2 glucose           -0.156           0.00608\n3 insulin           -0.0409          0.0115 \n4 mass              -0.0291          0.00747\n5 pedigree           0.00408         0.0148 \n6 pregnant          -0.0203          0.00815\n7 pressure          -0.00941         0.0105 \n8 triceps            0.0251          0.0166\nCode\nlibrary(ggplot2)\n\nggplot(lime_df_clean, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"red\") +\n  labs(title = \"Distribution of LIME Contributions\",\n       x = \"Variable\", y = \"Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(lime_summary_clean, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"blue\") +\n  labs(title = \"Standard Deviation of LIME Explanations\",\n       x = \"Variable\", y = \"Standard Deviation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\nCode\n# SHAP\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nshap_df &lt;- do.call(rbind, lapply(1:10, function(i) {\n  df_i &lt;- shap_results[[i]]\n  df_i$seed &lt;- i\n  df_i\n}))\n\nshap_df &lt;- shap_df |&gt;\n  rename(variable = feature,\n         contribution = phi) |&gt;\n  dplyr::select(variable, contribution, seed)\n\nshap_df &lt;- shap_df |&gt;\n  filter(variable != \"intercept\" & variable != \"\") |&gt;\n  mutate(variable = factor(variable))  \n\n\nshap_summary &lt;- shap_df |&gt;\n  group_by(variable) |&gt;\n  summarise(\n    mean_contribution = mean(contribution),\n    sd_contribution = sd(contribution),\n    .groups = \"drop\"\n  )\n\nprint(shap_summary)\n\n\n# A tibble: 8 × 3\n  variable mean_contribution sd_contribution\n  &lt;fct&gt;                &lt;dbl&gt;           &lt;dbl&gt;\n1 age               5.20e-18         0.0585 \n2 glucose           8.33e-18         0.169  \n3 insulin           1.56e-18         0.0168 \n4 mass             -2.78e-18         0.0597 \n5 pedigree          2.43e-18         0.0166 \n6 pregnant         -8.67e-19         0.0306 \n7 pressure         -5.64e-19         0.00953\n8 triceps           8.67e-19         0.0231\nCode\nggplot(shap_df, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"green\") +\n  labs(title = \"Distribution of SHAP Contributions\",\n       x = \"Variable\", y = \"Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(shap_summary, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"yellow\") +\n  labs(title = \"Standard Deviation of SHAP Explanations\",\n       x = \"Variable\", y = \"Standard Deviation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step 1</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability-analysis---random-forest",
    "href": "step1.html#repeatability-analysis---random-forest",
    "title": "2  Step 1",
    "section": "2.2 Repeatability Analysis - Random Forest",
    "text": "2.2 Repeatability Analysis - Random Forest\nIn this part, we trained a Random Forest classifier 10 times on the PimaIndiansDiabetes dataset. For each model, we used LIME and SHAP to explain the prediction of the same run Then we aggregated the feature-level contributions and calculated the mean and standard deviation across all runs to assess the repeatability of explanations.\nThe results show that LIME explanations vary significantly across runs, especially for features with moderate importance such as triceps and pedigree. In contrast, features like glucose exhibited relatively low standard deviation and consistent direction of contribution. For SHAP, although the mean contribution for most features was close to zero, the standard deviation was large for key features like glucose and mass. This suggests that SHAP may be more balanced in average attribution but more sensitive to model perturbations.\nIn summary, both LIME and SHAP explanations were affected by the choice of random seed, demonstrating non-negligible variability in feature attribution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step 1</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability---logistic-regression",
    "href": "step1.html#repeatability---logistic-regression",
    "title": "2  Step 1",
    "section": "2.3 Repeatability - Logistic Regression",
    "text": "2.3 Repeatability - Logistic Regression\n\n\nCode\n#install.packages(c(\"mlbench\", \"caret\", \"DALEX\", \"DALEXtra\", \"iml\", \"lime\",\"dplyr\", \"ggplot2\", \"ggpubr\"), dependencies = TRUE)\n\nlibrary(mlbench)\nlibrary(caret)\nlibrary(DALEX)\nlibrary(DALEXtra)\nlibrary(iml)\nlibrary(lime)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n\n\nCode\nmodel_type.glm &lt;- function(x, ...) \"classification\"\npredict_model.glm &lt;- function(x, newdata, ...) {\n  preds &lt;- predict(x, newdata, type = \"response\")\n  data.frame(`No` = 1 - preds, `Yes` = preds)\n}\n\n\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\ndf$diabetes &lt;- factor(df$diabetes)\nX &lt;- df[, -ncol(df)]\ny &lt;- df$diabetes\n\nlime_contributions &lt;- list()\nshap_contributions &lt;- list()\n\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\n\nIn the first step, we evaluate the repeatability of LIME explanations by repeatedly training the model and generating explanations to understand their consistency.\nWe conduct self-sampling (bootstrap) on the training data to obtain multiple different training sets. Each time, we train the logistic regression model on the newly sampled data and run the LIME interpretation model prediction with the same test samples, and calculate the standard deviation of the contribution value of each feature in multiple runs.\n\n\nCode\nn_runs &lt;- 10\nlime_contribs &lt;- list()\n\nfor (i in 1:n_runs) {\n  boot_idx &lt;- sample(nrow(train_data), replace = TRUE)\n  boot_train &lt;- train_data[boot_idx, ]\n\n  logit_model &lt;- glm(diabetes ~ ., data = boot_train, family = binomial)\n\n  explainer &lt;- DALEX::explain(\n    model = logit_model,\n    data = boot_train[, -ncol(boot_train)],\n    y = as.numeric(boot_train$diabetes == \"pos\"),\n    label = paste0(\"glm_\", i),\n    predict_function = function(m, d) predict(m, d, type = \"response\")\n  )\n\n  lime_result &lt;- predict_parts(explainer, new_observation = test_data[1, -ncol(test_data)],\n                               type = \"break_down\")\n  lime_df &lt;- lime_result %&gt;%\n    filter(variable != \"intercept\") %&gt;%\n    select(variable, contribution)\n\n  lime_contribs[[i]] &lt;- lime_df\n}\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001490493 , mean =  0.3593496 , max =  0.9958617  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9534075 , mean =  -3.825606e-13 , max =  0.9856082  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_2 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.002668286 , mean =  0.3219512 , max =  0.978063  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8972503 , mean =  -2.019543e-13 , max =  0.9888889  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_3 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.002188936 , mean =  0.3447154 , max =  0.9860676  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8505019 , mean =  -5.274643e-15 , max =  0.9907279  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_4 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0004471588 , mean =  0.3317073 , max =  0.9980853  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.94405 , mean =  -1.474228e-10 , max =  0.9947934  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_5 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001453277 , mean =  0.3495935 , max =  0.9830114  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9333344 , mean =  -1.372175e-15 , max =  0.9933661  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_6 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.004191668 , mean =  0.3447154 , max =  0.9763506  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9545668 , mean =  -1.842623e-13 , max =  0.9958083  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_7 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.004528123 , mean =  0.3252033 , max =  0.9956203  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9475119 , mean =  -9.300239e-15 , max =  0.983609  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_8 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.003427715 , mean =  0.3560976 , max =  0.9924571  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9353339 , mean =  -3.610481e-18 , max =  0.9104446  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_9 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.007305473 , mean =  0.3658537 , max =  0.9946787  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9522851 , mean =  -6.818368e-14 , max =  0.9577366  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_10 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  function(m, d) predict(m, d, type = \"response\") \n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001906982 , mean =  0.3284553 , max =  0.9547832  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9030004 , mean =  -2.326684e-15 , max =  0.9934916  \n  A new explainer has been created!  \n\n\nCode\nlime_summary &lt;- bind_rows(lime_contribs, .id = \"run\") %&gt;%\n  group_by(variable) %&gt;%\n  summarise(sd_contribution = sd(contribution), .groups = \"drop\")\n\nlibrary(ggplot2)\nggplot(lime_summary, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"LIME Repeatability (SD of Contributions)\", x = \"Variable\", y = \"SD\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step 1</span>"
    ]
  },
  {
    "objectID": "step1.html#repeatability-analysis---logistic-regression",
    "href": "step1.html#repeatability-analysis---logistic-regression",
    "title": "2  Step 1",
    "section": "2.4 Repeatability Analysis - Logistic Regression",
    "text": "2.4 Repeatability Analysis - Logistic Regression\nThe column height indicates the standard deviation of the LIME contribution for each feature across different model runs. A higher column means greater variation in the feature’s attribution, indicating worse repeatability, a lower column implies more consistent contributions, indicating greater stability.\nWe found that LIME explanations were more sensitive to training data changes for features like mass and glucose, which had the highest standard deviations in contribution across runs. However, features such as pressure, age, and pedigree showed very low variability, with consistently stable contributions across repeated model trainings. This suggests that for these features, LIME explanations are highly repeatable and robust.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Step 1</span>"
    ]
  },
  {
    "objectID": "step2.html",
    "href": "step2.html",
    "title": "3  Step 2",
    "section": "",
    "text": "3.1 Change Training Dataset Size - Random Forest\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(DALEX)\n\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\n\nCode\nlibrary(iml)\nlibrary(mlbench)\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:DALEX':\n\n    explain\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\ndf$diabetes &lt;- factor(df$diabetes, levels = c(\"neg\", \"pos\"))  \n\nset.seed(5293)\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data_full &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\nx_test &lt;- test_data[1, -ncol(test_data)]\n\nfractions &lt;- c(0.1, 0.3, 0.5, 1.0)\nlime_by_size &lt;- list()\nshap_by_size &lt;- list()\nCode\nfor (f in fractions) {\n  set.seed(2025)\n  n_sample &lt;- floor(nrow(train_data_full) * f)\n  sampled_idx &lt;- sample(nrow(train_data_full), n_sample)\n  sub_train &lt;- train_data_full[sampled_idx, ]\n  \n  X_sub &lt;- sub_train[, -ncol(sub_train)]\n  y_sub &lt;- sub_train$diabetes\n\n  rf_model &lt;- randomForest(x = X_sub, y = y_sub, ntree = 100)\n\n  # LIME\n explainer &lt;- DALEX::explain(\n  model = rf_model,\n  data = X_sub,\n  y = NULL,  \n  predict_function = function(m, d) predict(m, d, type = \"prob\")[, 2],\n  label = paste0(\"RF_\", f*100, \"%\"),\n  verbose = FALSE\n)\n\n  lime_expl &lt;- predict_parts(\n    explainer,\n    new_observation = x_test,\n    type = \"break_down\"\n  )\n  lime_by_size[[as.character(f)]] &lt;- lime_expl\n\n  # SHAP\n  predictor &lt;- Predictor$new(rf_model, data = X_sub, y = y_sub, type = \"prob\")\n  shap &lt;- Shapley$new(predictor, x.interest = x_test)\n  shap_by_size[[as.character(f)]] &lt;- shap$results\n}\nCode\n# LIME\n\npredict_function = function(m, d) {\n  predict(m, d, type = \"prob\")[, \"pos\"]\n}\n\n\nexplainer &lt;- DALEX::explain(\n  model = rf_model,\n  data = X_sub,\n  y = NULL,\n  predict_function = function(m, d) predict(m, d, type = \"prob\")[, \"pos\"],\n  label = paste0(\"RF_\", f * 100, \"%\"),\n  verbose = FALSE\n)\n\nlime_df &lt;- predict_parts(\n  explainer,\n  new_observation = x_test,\n  type = \"break_down\"  \n)\n\nprint(lime_df)\n\n\n                          contribution\nRF_100%: intercept               0.346\nRF_100%: glucose = 85           -0.136\nRF_100%: mass = 26.6            -0.039\nRF_100%: age = 31                0.068\nRF_100%: pregnant = 1           -0.013\nRF_100%: triceps = 29            0.035\nRF_100%: pressure = 66           0.008\nRF_100%: pedigree = 0.351        0.009\nRF_100%: insulin = 0            -0.028\nRF_100%: prediction              0.250\nCode\nlibrary(dplyr)\n\nlime_all &lt;- purrr::map2_dfr(\n  lime_by_size,\n  names(lime_by_size),\n  ~ .x |&gt;\n    filter(variable != \"intercept\", variable != \"\", variable != \"prediction\") |&gt;\n    mutate(variable = factor(variable),\n           fraction = .y)\n)\n\n\nlime_sd_summary &lt;- lime_all |&gt;\n  group_by(variable) |&gt;\n  summarise(sd_contribution = sd(contribution), .groups = \"drop\")\n\n\nlime_sd_summary |&gt;\n  arrange(desc(sd_contribution)) |&gt;\n  print()\n\n\n# A tibble: 8 × 2\n  variable         sd_contribution\n  &lt;fct&gt;                      &lt;dbl&gt;\n1 age = 31                  0.0375\n2 pregnant = 1              0.0249\n3 glucose = 85              0.0205\n4 pressure = 66             0.0185\n5 triceps = 29              0.0144\n6 mass = 26.6               0.0137\n7 pedigree = 0.351          0.0110\n8 insulin = 0               0.0109\nCode\nlibrary(ggplot2)\n\nggplot(lime_sd_summary, aes(x = reorder(variable, sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"grey\") +\n  coord_flip() +\n  labs(\n    title = \"Stability of LIME Explanations\",\n    x = \"Variable\",\n    y = \"Standard Deviation\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\n\nCode\nggplot(lime_all, aes(x = reorder(variable, contribution, FUN = median), y = contribution)) +\n  geom_boxplot(fill = \"orange\", color = \"black\", outlier.shape = 21) +\n  coord_flip() +\n  labs(\n    title = \"Distribution of LIME Contributions\",\n    x = \"Variable\",\n    y = \"Contribution\"\n  ) +\n  theme_minimal(base_size = 14)\nCode\n# SHAP\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nshap_df &lt;- bind_rows(lapply(names(shap_by_size), function(name) {\n  df &lt;- shap_by_size[[name]]\n  df$train_size &lt;- name\n  return(df)\n}))\n\nshap_df_clean &lt;- shap_df |&gt;\n  rename(variable = feature,\n         contribution = phi) |&gt;\n  filter(variable != \"intercept\", variable != \"\") |&gt;\n  mutate(variable = factor(variable))\n\nshap_sd_summary &lt;- shap_df_clean |&gt;\n  group_by(variable) |&gt;\n  summarise(sd_contribution = sd(contribution), .groups = \"drop\")\n\nprint(shap_sd_summary |&gt; arrange(desc(sd_contribution)))\n\n\n# A tibble: 8 × 2\n  variable sd_contribution\n  &lt;fct&gt;              &lt;dbl&gt;\n1 glucose           0.114 \n2 age               0.0749\n3 mass              0.0741\n4 pregnant          0.0557\n5 triceps           0.0249\n6 pressure          0.0205\n7 pedigree          0.0145\n8 insulin           0.0141\nCode\nggplot(shap_df_clean, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"pink\") +\n  labs(title = \"Distribution of SHAP Contributions\",\n       x = \"Variable\", y = \"Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(shap_sd_summary, aes(x = reorder(variable, -sd_contribution), y = sd_contribution)) +\n  geom_col(fill = \"brown\") +\n  labs(title = \"Standard Deviation of SHAP Contributions\",\n       x = \"Variable\", y = \"Standard Deviation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Step 2</span>"
    ]
  },
  {
    "objectID": "step2.html#change-training-dataset-size-analysis---random-forest",
    "href": "step2.html#change-training-dataset-size-analysis---random-forest",
    "title": "3  Step 2",
    "section": "3.2 Change Training Dataset Size Analysis - Random Forest",
    "text": "3.2 Change Training Dataset Size Analysis - Random Forest\nWe trained random forest models on 10%, 30%, 50%, and 100% subsets of the PimaIndiansDiabetes dataset. For each subset, LIME and SHAP explanations were generated for the same run, and the standard deviation of contributions was used to quantify explanation variability.\nLIME results show that variables such as glucose and mass have the lowest standard deviation, so they are the most stable explanations. However, triceps and pedigree show higher variability across training sizes. This suggests that features with stronger signal or clearer influence on the prediction remain stable even when trained on smaller datasets. SHAP results similar trends but exhibit overall greater variability. Notably, the glucose feature shows the highest standard deviation in SHAP, indicating that its attribution is more sensitive to the training data size.\nIn conclusion, increasing the size of training data generally leads to more stable and consistent feature attributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Step 2</span>"
    ]
  },
  {
    "objectID": "step2.html#change-training-dataset-size---logistic-regression",
    "href": "step2.html#change-training-dataset-size---logistic-regression",
    "title": "3  Step 2",
    "section": "3.3 Change Training Dataset Size - Logistic Regression",
    "text": "3.3 Change Training Dataset Size - Logistic Regression\nWe examined how the size of the training dataset affects the variability of LIME explanations, particularly across different feature-value pairs (e.g., glucose = 101, mass = 31.4). The bar chart displays the standard deviation (SD) of LIME contributions at four training sizes: 10%, 30%, 50%, and 100%.\n\n\nCode\nmodel_type.glm &lt;- function(x, ...) \"classification\"\npredict_model.glm &lt;- function(x, newdata, ...) {\n  preds &lt;- predict(x, newdata, type = \"response\")\n  data.frame(`No` = 1 - preds, `Yes` = preds)\n}\n\n\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\ndf$diabetes &lt;- factor(df$diabetes)\nX &lt;- df[, -ncol(df)]\ny &lt;- df$diabetes\n\nlime_contributions &lt;- list()\nshap_contributions &lt;- list()\n\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\n\n\n\nCode\nsizes &lt;- c(0.1, 0.3, 0.5, 1.0)\nn_obs &lt;- 5  \nn_repeats &lt;- 5 \n\nsize_results &lt;- list()\n\nfor (size in sizes) {\n  for (rep in 1:n_repeats) {\n    \n    if (size == 1.0) {\n      sub_train &lt;- train_data\n    } else {\n      sub_idx &lt;- sample(nrow(train_data), size = floor(nrow(train_data) * size))\n      sub_train &lt;- train_data[sub_idx, ]\n    }\n\n    logit_model &lt;- glm(diabetes ~ ., data = sub_train, family = binomial)\n\n    explainer &lt;- DALEX::explain(\n      logit_model,\n      data = sub_train[, -ncol(sub_train)],\n      y = as.numeric(sub_train$diabetes == \"pos\"),\n      label = paste0(\"glm_size_\", size)\n    )\n\n    for (i in 1:n_obs) {\n      lime_expl &lt;- predict_parts(\n        explainer,\n        new_observation = test_data[i, -ncol(test_data)],\n        type = \"break_down\"\n      )\n      lime_expl$size &lt;- as.character(size)\n      lime_expl$rep &lt;- rep\n      lime_expl$obs &lt;- i\n      size_results[[length(size_results) + 1]] &lt;- lime_expl\n    }\n  }\n}\n\n\nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.1 \n  -&gt; data              :  61  rows  8  cols \n  -&gt; target variable   :  61  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.01586211 , mean =  0.3442623 , max =  0.9887167  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.7908998 , mean =  -1.719104e-11 , max =  0.8439318  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.1 \n  -&gt; data              :  61  rows  8  cols \n  -&gt; target variable   :  61  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001370974 , mean =  0.4262295 , max =  0.9975288  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.7311193 , mean =  4.167347e-13 , max =  0.8912169  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.1 \n  -&gt; data              :  61  rows  8  cols \n  -&gt; target variable   :  61  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001396533 , mean =  0.4262295 , max =  0.999766  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8829128 , mean =  1.991121e-15 , max =  0.860971  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.1 \n  -&gt; data              :  61  rows  8  cols \n  -&gt; target variable   :  61  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0002644932 , mean =  0.3278689 , max =  0.9819968  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.792514 , mean =  -5.28556e-15 , max =  0.8393045  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.1 \n  -&gt; data              :  61  rows  8  cols \n  -&gt; target variable   :  61  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.007157103 , mean =  0.3442623 , max =  0.979401  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.7496494 , mean =  3.022316e-12 , max =  0.9041704  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.3 \n  -&gt; data              :  184  rows  8  cols \n  -&gt; target variable   :  184  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0003898785 , mean =  0.3695652 , max =  0.9901829  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9616683 , mean =  -8.622614e-12 , max =  0.9118897  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.3 \n  -&gt; data              :  184  rows  8  cols \n  -&gt; target variable   :  184  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.009355782 , mean =  0.3315217 , max =  0.9668781  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.7974013 , mean =  -5.974652e-14 , max =  0.9745445  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.3 \n  -&gt; data              :  184  rows  8  cols \n  -&gt; target variable   :  184  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0009809545 , mean =  0.326087 , max =  0.9856423  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8987656 , mean =  -1.163468e-11 , max =  0.8936537  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.3 \n  -&gt; data              :  184  rows  8  cols \n  -&gt; target variable   :  184  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.0009854849 , mean =  0.3315217 , max =  0.9882061  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9552123 , mean =  -2.664249e-11 , max =  0.8874289  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.3 \n  -&gt; data              :  184  rows  8  cols \n  -&gt; target variable   :  184  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.009806256 , mean =  0.3315217 , max =  0.9836747  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.7999698 , mean =  -2.99003e-14 , max =  0.9901937  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.5 \n  -&gt; data              :  307  rows  8  cols \n  -&gt; target variable   :  307  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001861421 , mean =  0.3485342 , max =  0.9941528  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9486772 , mean =  -2.235641e-14 , max =  0.9856247  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.5 \n  -&gt; data              :  307  rows  8  cols \n  -&gt; target variable   :  307  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.003849884 , mean =  0.3192182 , max =  0.938876  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.8498261 , mean =  -1.275446e-15 , max =  0.9838009  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.5 \n  -&gt; data              :  307  rows  8  cols \n  -&gt; target variable   :  307  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.000849326 , mean =  0.3420195 , max =  0.9861281  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9336583 , mean =  -2.584836e-13 , max =  0.9683198  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.5 \n  -&gt; data              :  307  rows  8  cols \n  -&gt; target variable   :  307  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.002316585 , mean =  0.3745928 , max =  0.9909388  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9408385 , mean =  -8.876179e-15 , max =  0.9686949  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_0.5 \n  -&gt; data              :  307  rows  8  cols \n  -&gt; target variable   :  307  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001331084 , mean =  0.3289902 , max =  0.9995719  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9344751 , mean =  -5.301455e-11 , max =  0.995086  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001416797 , mean =  0.3495935 , max =  0.9943172  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9518769 , mean =  -4.526072e-14 , max =  0.989687  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001416797 , mean =  0.3495935 , max =  0.9943172  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9518769 , mean =  -4.526072e-14 , max =  0.989687  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001416797 , mean =  0.3495935 , max =  0.9943172  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9518769 , mean =  -4.526072e-14 , max =  0.989687  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001416797 , mean =  0.3495935 , max =  0.9943172  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9518769 , mean =  -4.526072e-14 , max =  0.989687  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -&gt; model label       :  glm_size_1 \n  -&gt; data              :  615  rows  8  cols \n  -&gt; target variable   :  615  values \n  -&gt; predict function  :  yhat.glm  will be used (  default  )\n  -&gt; predicted values  :  No value for predict function target column. (  default  )\n  -&gt; model_info        :  package stats , ver. 4.4.2 , task classification (  default  ) \n  -&gt; predicted values  :  numerical, min =  0.001416797 , mean =  0.3495935 , max =  0.9943172  \n  -&gt; residual function :  difference between y and yhat (  default  )\n  -&gt; residuals         :  numerical, min =  -0.9518769 , mean =  -4.526072e-14 , max =  0.989687  \n  A new explainer has been created!  \n\n\nCode\nsize_df &lt;- bind_rows(size_results) %&gt;%\n  filter(!is.na(variable), variable != \"intercept\")\n\nrobust_summary &lt;- size_df %&gt;%\n  group_by(size, variable) %&gt;%\n  summarise(sd_contribution = sd(contribution, na.rm = TRUE), .groups = 'drop')\n\nggplot(robust_summary, aes(x = variable, y = sd_contribution, fill = size)) +\n  geom_col(position = position_dodge()) +\n  labs(title = \"LIME Contribution Variability across Training Sizes\",\n       x = \"Variable\", y = \"Standard Deviation of Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Step 2</span>"
    ]
  },
  {
    "objectID": "step2.html#change-training-dataset-size-analysis---logistic-regression",
    "href": "step2.html#change-training-dataset-size-analysis---logistic-regression",
    "title": "3  Step 2",
    "section": "3.4 Change Training Dataset Size Analysis - Logistic Regression",
    "text": "3.4 Change Training Dataset Size Analysis - Logistic Regression\nOverall trend:\nAs the training size increases, the standard deviation (SD) of most feature contributions tends to decrease, indicating improved explanation stability. In general, red bars (10%) are taller than green (30%), blue (50%), and purple (100%), especially for features like glucose. Some features remain low throughout, but most show reduced SD with more data.\nHowever, for the feature prediction, the SD remains consistently high across all training sizes, suggesting inherent instability in interpreting the predicted outcome itself.\nHigh-variability pairs:\nAmong all variables, glucose = 166 shows the highest standard deviation at small training sizes, suggesting that its LIME contribution is highly sensitive to training data fluctuations. This variability decreases as training size increases, indicating improved robustness.\nThe item prediction, which represents the explanation of the predicted class itself, consistently exhibits the highest SD across all training sizes.\nStable feature-value pairs:\nPairs like pressure = 66, pressure = 72, triceps = 29, triceps = 32, and insulin = 88 maintain low variability throughout, indicating strong robustness in LIME’s local explanations for these values.\nLIME explanations become more reliable with larger training sets. When using small training sizes (especially &lt; 30%), explanations of some feature values may be highly volatile and potentially misleading. For critical medical features like glucose and BMI, we recommend using at least 50% of the data to ensure stable and trustworthy interpretations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Step 2</span>"
    ]
  },
  {
    "objectID": "step3.html",
    "href": "step3.html",
    "title": "4  Step 3",
    "section": "",
    "text": "4.1 Consistency - Random Forest\nCode\nlibrary(randomForest)\n\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nCode\nlibrary(DALEX)\n\n\nWelcome to DALEX (version: 2.4.3).\nFind examples and detailed introduction at: http://ema.drwhy.ai/\n\n\nCode\nlibrary(DALEXtra)\nlibrary(mlbench)\nlibrary(caret)\n\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:randomForest':\n\n    margin\n\n\nLoading required package: lattice\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:DALEX':\n\n    explain\n\n\nThe following object is masked from 'package:randomForest':\n\n    combine\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(ggplot2)\n\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\ndf$diabetes &lt;- factor(df$diabetes, levels = c(\"neg\", \"pos\")) \n\n\nset.seed(5293)\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\nX &lt;- train_data[, -ncol(train_data)]\ny &lt;- train_data$diabetes\nCode\n# logistic regression\nlogit_model &lt;- glm(diabetes ~ ., data = train_data, family = binomial)\nlogit_coef &lt;- coef(logit_model)[-1]  \ncoef_df &lt;- tibble(variable = names(logit_coef),\n                  logistic_coef = as.numeric(logit_coef))\n\n# RandomForest + LIME\nlime_results &lt;- list()\n\nfor (i in 1:10) {\n  set.seed(i)\n  rf_model &lt;- randomForest(x = X, y = y, ntree = 100)\n\n  explainer &lt;- DALEX::explain(\n    model = rf_model,\n    data = X,\n    y = NULL, \n    predict_function = function(m, d) predict(m, d, type = \"prob\")[, 2],\n    label = paste0(\"rf_seed_\", i),\n    verbose = FALSE\n  )\n\n  lime_expl &lt;- predict_parts(\n    explainer,\n    new_observation = test_data[1, -ncol(test_data)],\n    type = \"break_down\"\n  )\n\n  lime_results[[i]] &lt;- lime_expl\n}\nCode\nlibrary(stringr)\n\ncoef_df &lt;- tibble(\n  variable_clean = names(logit_coef),\n  logistic_coef = as.numeric(logit_coef)\n)\n\nlime_df &lt;- bind_rows(lime_results)\n\nlime_df &lt;- lime_df |&gt;\n  mutate(variable_clean = str_trim(str_extract(variable, \"^[^=]+\")))\n\nlime_mean_df &lt;- lime_df |&gt;\n  group_by(variable_clean) |&gt;\n  summarise(mean_lime = mean(contribution), .groups = \"drop\")\n\nconsistency_df &lt;- inner_join(coef_df, lime_mean_df, by = \"variable_clean\") |&gt;\n  mutate(abs_diff = abs(logistic_coef - mean_lime))\n\nprint(consistency_df)\n\n\n# A tibble: 8 × 4\n  variable_clean logistic_coef mean_lime abs_diff\n  &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 pregnant            0.148     -0.0203   0.169  \n2 glucose             0.0367    -0.156    0.193  \n3 pressure           -0.0123    -0.00941  0.00288\n4 triceps            -0.000847   0.0251   0.0259 \n5 insulin            -0.000418  -0.0409   0.0405 \n6 mass                0.0895    -0.0291   0.119  \n7 pedigree            1.11       0.00408  1.11   \n8 age                 0.00774    0.0457   0.0380\nCode\nlibrary(ggplot2)\n\nggplot(consistency_df, aes(x = mean_lime, y = logistic_coef, label = variable_clean)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray\") +\n  geom_text(nudge_y = 0.05, size = 3.5) +\n  labs(title = \"Comparison of LIME Mean Contribution vs Logistic Coefficient\",\n       x = \"LIME Mean Contribution\",\n       y = \"Logistic Regression Coefficient\") +\n  theme_minimal()\nCode\nggplot(consistency_df, aes(x = reorder(variable_clean, -abs_diff), y = abs_diff)) +\n  geom_col(fill = \"red\") +\n  labs(title = \"Absolute Difference Between LIME and Logistic Coefficients\",\n       x = \"Variable\", y = \"Absolute Difference\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 3</span>"
    ]
  },
  {
    "objectID": "step3.html#consistency-analysis---random-forest",
    "href": "step3.html#consistency-analysis---random-forest",
    "title": "4  Step 3",
    "section": "4.2 Consistency Analysis - Random Forest",
    "text": "4.2 Consistency Analysis - Random Forest\nwe trained a logistic regression model using the same dataset and compared the model’s coefficients with the mean LIME contributions aggregated across multiple runs. Since LIME is a local explanation method, while logistic regression provides global coefficients, their alignment can indicate whether local interpretability methods reflect global trends.\nFrom the scatter plot of logistic regression coefficients vs LIME mean contributions, features such as age, pressure, and triceps show small absolute differences between the logistic coefficient and LIME average contribution, suggesting good consistency. In contrast, pedigree exhibits a large deviation, indicating that LIME’s local explanations for this variable may not align well with the global behavior captured by the logistic model.\nOverall, the analysis suggests that while LIME explanations are partially aligned with global model behavior, they may deviate for variables with complex influence.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 3</span>"
    ]
  },
  {
    "objectID": "step3.html#consistency---logistic-regression",
    "href": "step3.html#consistency---logistic-regression",
    "title": "4  Step 3",
    "section": "4.3 Consistency - Logistic Regression",
    "text": "4.3 Consistency - Logistic Regression\nIn the third step, we examined the sensitivity of the model interpretation among different individuals. We used a fixed logistic regression model, applied LIME to 10 test samples and plotted a box plot of feature contributions.\n\n\nCode\nmodel_type.glm &lt;- function(x, ...) \"classification\"\npredict_model.glm &lt;- function(x, newdata, ...) {\n  preds &lt;- predict(x, newdata, type = \"response\")\n  data.frame(`No` = 1 - preds, `Yes` = preds)\n}\n\n\ndata(PimaIndiansDiabetes)\ndf &lt;- na.omit(PimaIndiansDiabetes)\nset.seed(5293)\ndf$diabetes &lt;- factor(df$diabetes)\nX &lt;- df[, -ncol(df)]\ny &lt;- df$diabetes\n\nlime_contributions &lt;- list()\nshap_contributions &lt;- list()\n\n\ntrain_idx &lt;- createDataPartition(df$diabetes, p = 0.8, list = FALSE)\ntrain_data &lt;- df[train_idx, ]\ntest_data &lt;- df[-train_idx, ]\n\n\n\n\nCode\nlogit_model &lt;- glm(diabetes ~ ., data = train_data, family = binomial)\n\nlime_global &lt;- lime::lime(\n  x = train_data[, -ncol(train_data)],\n  model = logit_model\n)\n\nlime_explanations &lt;- lime::explain(\n  x = test_data[1:10, -ncol(test_data)],\n  explainer = lime_global,\n  n_features = 8,\n  n_labels = 1\n)\n\nsensitivity_df &lt;- lime_explanations %&gt;%\n  select(case, feature, feature_weight) %&gt;%\n  rename(variable = feature, contribution = feature_weight)\n\nlibrary(ggplot2)\n\nggplot(sensitivity_df, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"#69b3a2\", alpha = 0.7) +\n  labs(\n    title = \"LIME Sensitivity: Variability Across Test Samples\",\n    x = \"Variable\",\n    y = \"LIME Contribution\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\npredictor &lt;- iml::Predictor$new(model = logit_model, data = train_data[, -ncol(train_data)], y = train_data$diabetes)\nsensitivity_shap &lt;- list()\n\nfor (i in 1:10) {\n  shap &lt;- iml::Shapley$new(predictor, x.interest = test_data[i, -ncol(test_data)])\n  df &lt;- shap$results %&gt;% mutate(case = i)\n  sensitivity_shap[[i]] &lt;- df\n}\n\nshap_df &lt;- bind_rows(sensitivity_shap) %&gt;%\n  rename(variable = feature, contribution = phi)\n\nggplot(shap_df, aes(x = variable, y = contribution)) +\n  geom_boxplot(fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"SHAP Sensitivity: Variability Across Test Samples\",\n       x = \"Variable\", y = \"SHAP Contribution\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 3</span>"
    ]
  },
  {
    "objectID": "step3.html#consistency-analysis---logistic-regression",
    "href": "step3.html#consistency-analysis---logistic-regression",
    "title": "4  Step 3",
    "section": "4.4 Consistency Analysis - Logistic Regression",
    "text": "4.4 Consistency Analysis - Logistic Regression\n\nLIME chart:\n\nGlucose: The box plot has the largest span (0.1-0.4+), indicating that the interpreted value is sensitive to the changes of the test case.\nThere are also obvious changes in “mass” and “pregnant”, and the contribution direction will change.\nAge, Triceps, Insulin: The box is very narrow and the values are almost fixed, indicating that individual changes have no significant impact on the interpretation.\n\nSHAP Diagram:\n\nGlucose remains the most sensitive variable, with a wider range of variation than LIME (-1 to 1.2).\nThe contribution of SHAP to most variables (such as age, insulin, triceps) is almost constant.\nCompared with LIME, the directions are consistent (such as glucose, mass), but the variation amplitude is greater, showing the characteristic that the local interpretation of SHAP is more sensitive.\nLIME and SHAP explanations show a high degree of consistency in identifying the most sensitive and least sensitive features in different test samples. Although SHAP tends to exhibit a large range of variation due to its theoretical design, the overall ranking and directional contribution of features are closely consistent with the LIME results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Step 3</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "In this study, we evaluated the stability of Random Forest and Logistic Regression models with local explanation methods, LIME and SHAP, on the PimaIndiansDiabetes dataset.\nThrough repeatability analysis, we found that LIME explanations were moderately sensitive to random initialization. However, SHAP exhibited more balanced mean contributions but higher variance for certain influential features, suggesting greater sensitivity to model perturbations. In the training dataset size analysis, we observed that increasing training size generally improved the stability of both LIME and SHAP explanations. Logistic regression models yielded more consistent LIME explanations, indicating stronger robustness to sample size fluctuations. Finally, in the consistency analysis, we compared the global coefficients of logistic regression to the average local contributions from LIME.While many features, such as age and pressure, aligned well between global and local interpretations, others like pedigree showed divergence, highlighting the limits of local explanations in capturing complex global behavior. SHAP demonstrated high sensitivity and directional agreement with LIME, but often with greater attribution magnitude.\nOverall, the results suggest that while both LIME and SHAP are valuable tools for interpreting model predictions, their stability and alignment with global trends can vary considerably depending on the model, data quantity, and specific feature characteristics.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]